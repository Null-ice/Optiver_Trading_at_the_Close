{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdfce298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:43:23.103168Z",
     "iopub.status.busy": "2024-07-03T03:43:23.102479Z",
     "iopub.status.idle": "2024-07-03T03:45:13.838720Z",
     "shell.execute_reply": "2024-07-03T03:45:13.837494Z"
    },
    "papermill": {
     "duration": 110.750436,
     "end_time": "2024-07-03T03:45:13.845849",
     "exception": false,
     "start_time": "2024-07-03T03:43:23.095413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1.25 s, sys: 294 ms, total: 1.55 s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from IPython.display import clear_output;\n",
    "import gc;\n",
    "\n",
    "!pip install /kaggle/input/lightgbm410/lightgbm-4.1.0-py3-none-manylinux_2_28_x86_64.whl -q;\n",
    "!pip install /kaggle/input/xgboost-2-0-0-whl/xgboost-2.0.2-py3-none-manylinux2014_x86_64.whl -q;\n",
    "!pip install /kaggle/input/catboost-1-2-2/catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl -q;\n",
    "\n",
    "clear_output();\n",
    "print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e50ea4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:13.857066Z",
     "iopub.status.busy": "2024-07-03T03:45:13.856506Z",
     "iopub.status.idle": "2024-07-03T03:45:20.158779Z",
     "shell.execute_reply": "2024-07-03T03:45:20.157717Z"
    },
    "papermill": {
     "duration": 6.310049,
     "end_time": "2024-07-03T03:45:20.160801",
     "exception": false,
     "start_time": "2024-07-03T03:45:13.850752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current CatBoost version = 1.2.2\n",
      "Current LightGBM version = 4.1.0\n",
      "\n",
      "\n",
      "CPU times: user 3.15 s, sys: 667 ms, total: 3.81 s\n",
      "Wall time: 6.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import ctypes, gc, os, time, warnings, joblib, lightgbm as lgb, catboost, numpy as np, pandas as pd, polars as pl;\n",
    "from warnings import simplefilter;\n",
    "warnings.filterwarnings(\"ignore\");\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning);\n",
    "\n",
    "from itertools import combinations;\n",
    "from pprint import pprint;\n",
    "from catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm;\n",
    "from xgboost import XGBRegressor;\n",
    "from numba import njit, prange, jit;\n",
    "from sklearn.metrics import mean_absolute_error;\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit;\n",
    "from colorama import Fore, Style, init;\n",
    "\n",
    "clear_output();\n",
    "\n",
    "print(f\"\\nCurrent CatBoost version = {catboost.__version__}\");\n",
    "print(f\"Current LightGBM version = {lgb.__version__}\\n\");\n",
    "gc.collect();\n",
    "print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61a9aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:20.172499Z",
     "iopub.status.busy": "2024-07-03T03:45:20.171700Z",
     "iopub.status.idle": "2024-07-03T03:45:20.282433Z",
     "shell.execute_reply": "2024-07-03T03:45:20.281476Z"
    },
    "papermill": {
     "duration": 0.118446,
     "end_time": "2024-07-03T03:45:20.284275",
     "exception": false,
     "start_time": "2024-07-03T03:45:20.165829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 100 ms, sys: 0 ns, total: 100 ms\n",
      "Wall time: 99.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "version        = 27;\n",
    "inference_flag = 3;\n",
    "\n",
    "if inference_flag == 0:\n",
    "    is_offline = True;\n",
    "    is_train   = True;\n",
    "    is_infer   = False;\n",
    "    debug_flag = False;\n",
    "    \n",
    "elif inference_flag == 1:\n",
    "    is_offline = False;\n",
    "    is_train   = True;\n",
    "    is_infer   = True;\n",
    "    debug_flag = True;\n",
    "    \n",
    "elif inference_flag == 2:\n",
    "    is_offline = False;\n",
    "    is_train   = True;\n",
    "    is_infer   = True;\n",
    "    debug_flag = False;\n",
    "    \n",
    "elif inference_flag == 3:\n",
    "    is_offline = False;\n",
    "    is_train = False;\n",
    "    is_infer = True;\n",
    "    debug_flag = True;\n",
    "    \n",
    "max_lookback = np.nan;\n",
    "split_day    = 435;\n",
    "\n",
    "# DEBUG PARAMS\n",
    "streaming    = False;\n",
    "check_code   = False;\n",
    "test_runtime = False;\n",
    "\n",
    "# ENSEMBLE AND REFIT PARAMS\n",
    "use_lgb = True;\n",
    "use_cat = True;\n",
    "use_xgb = False;\n",
    "\n",
    "lgb_weight  = 0.50;\n",
    "cat_weight  = 0.50;\n",
    "xgb_weight  = 0;\n",
    "is_refit    = True;\n",
    "nb_refits   = 12;\n",
    "freq_refits = 6;\n",
    "wd_refits   = 450;\n",
    "\n",
    "# POST-PROCESSING PARAMETERS:- (zero_sum/ zero_mean/ NA):-\n",
    "postprocess = \"NA\";\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes);\n",
    "    step      = np.sum(prices)/np.sum(std_error);\n",
    "    out       = prices-std_error*step;\n",
    "    return out;\n",
    "\n",
    "def PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n",
    "    \"Prints color outputs using colorama using a text F-string\";\n",
    "    print(style + color + text + Style.RESET_ALL); \n",
    "    \n",
    "gc.collect();\n",
    "print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b033f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:20.295614Z",
     "iopub.status.busy": "2024-07-03T03:45:20.295344Z",
     "iopub.status.idle": "2024-07-03T03:45:47.369822Z",
     "shell.execute_reply": "2024-07-03T03:45:47.368857Z"
    },
    "papermill": {
     "duration": 27.082664,
     "end_time": "2024-07-03T03:45:47.372063",
     "exception": false,
     "start_time": "2024-07-03T03:45:20.289399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5204298, 20)\n",
      "\n",
      "CPU times: user 17.7 s, sys: 3.63 s, total: 21.3 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\");\n",
    "\n",
    "# Joining lagged targets:-\n",
    "last_targets = \\\n",
    "df[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"target\"]].\\\n",
    "rename(columns = {\"target\": \"prev_1_target\"});\n",
    "last_targets[\"date_id\"] = last_targets[\"date_id\"] + 1;\n",
    "df = df.merge(last_targets, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n",
    "del last_targets;\n",
    "gc.collect();\n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n",
    "\n",
    "last_targets_2 = \\\n",
    "df[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"target\"]].\\\n",
    "rename(columns = {\"target\": \"prev_2_target\"});\n",
    "last_targets_2[\"date_id\"] = last_targets_2[\"date_id\"] + 2\n",
    "df = df.merge(last_targets_2, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\")\n",
    "del last_targets_2;\n",
    "gc.collect();\n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n",
    "\n",
    "last_targets_3 = \\\n",
    "df[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"target\"]].\\\n",
    "rename(columns = {\"target\": \"prev_3_target\"})\n",
    "last_targets_3[\"date_id\"] = last_targets_3[\"date_id\"] + 3\n",
    "df = df.merge(last_targets_3, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\")\n",
    "del last_targets_3;\n",
    "gc.collect();\n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n",
    "\n",
    "df = df.dropna(subset=[\"target\", \"prev_1_target\", \"prev_2_target\", \"prev_3_target\"], how=\"any\");\n",
    "df.reset_index(drop=True, inplace=True);\n",
    "print(df.shape);\n",
    "\n",
    "if check_code:\n",
    "    df = df[df.date_id >= 375].reset_index(drop=True);\n",
    "\n",
    "gc.collect();\n",
    "print();\n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d1822b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:47.383965Z",
     "iopub.status.busy": "2024-07-03T03:45:47.383656Z",
     "iopub.status.idle": "2024-07-03T03:45:47.534598Z",
     "shell.execute_reply": "2024-07-03T03:45:47.533560Z"
    },
    "papermill": {
     "duration": 0.159243,
     "end_time": "2024-07-03T03:45:47.536565",
     "exception": false,
     "start_time": "2024-07-03T03:45:47.377322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 110 ms, sys: 850 Âµs, total: 111 ms\n",
      "Wall time: 111 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "####################################################################\n",
    "# Memory reduction:-\n",
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type to reduce memory usage.\n",
    "    \"\"\";\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and col != \"target\":\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == \"int\" or str(col_type)[:4] == \"uint\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    return df;\n",
    "\n",
    "####################################################################\n",
    "# RSI calculation:-\n",
    "def calculate_rsi(prices, period= 14):\n",
    "    rsi_values = np.zeros_like(prices);\n",
    "    for col in prange(prices.shape[1]):\n",
    "        price_data = prices[:, col];\n",
    "        delta      = np.zeros_like(price_data);\n",
    "        delta[1:]  = price_data[1:] - price_data[:-1];\n",
    "        gain       = pd.Series(np.where(delta > 0, delta, 0));\n",
    "        loss       = pd.Series(np.where(delta < 0, -delta, 0));\n",
    "        avg_gain   = gain.rolling(window=period,\n",
    "                                  min_periods=period).mean(engine='numba', engine_kwargs={\"parallel\": True});\n",
    "        avg_loss   = loss.rolling(window=period,\n",
    "                                  min_periods=period).mean(engine='numba', engine_kwargs={\"parallel\": True});\n",
    "        rs         = avg_gain / avg_loss;\n",
    "        rs         = rs.replace([np.inf, -np.inf], 1e-9);\n",
    "        rsi_values[:, col] = 100 - (100 / (1 + rs));\n",
    "    return rsi_values;\n",
    "\n",
    "def generate_rsi(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"];\n",
    "    sizes  = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"];\n",
    "\n",
    "    for stock_id, values in df.groupby(['stock_id'])[prices]:\n",
    "        columns = [f'rsi_{col}' for col in values.columns];\n",
    "        data    = calculate_rsi(values.values);\n",
    "        df.loc[values.index, columns] = data;\n",
    "    return df;\n",
    "\n",
    "def generate_all_features(df, global_ftre: dict,\n",
    "                          grouper_cols: list = ['stock_id'],\n",
    "                          roll_window : list = [1, 2, 3, 5, 10],\n",
    "                          ma_window   : list = [5, 10, 20],\n",
    "                          ewm_window  : list = [7, 30],\n",
    "                          wap_ftre_req: str = \"N\",\n",
    "                          **kwarg\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    This function generates all secondary features for the model\n",
    "\n",
    "    Note:-\n",
    "    1. We make MACD, EWM and BBbands, ASHI index for WAP only\n",
    "    2. We make SMA for certain columns\n",
    "    3. We use global features also\n",
    "    \"\"\";\n",
    "\n",
    "    cols      = [c for c in df.columns if c not in [\"row_id\", \"time_id\"]];\n",
    "    df        = df[cols];\n",
    "    prices    = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"];\n",
    "    sizes     = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"];\n",
    "    roll_cols = ['matched_size', 'imbalance_size', 'reference_price',\n",
    "                 'ask_price', 'bid_price', 'ask_size', 'bid_size', 'wap', 'near_price', 'far_price']\n",
    "    ma_cols   = ['imbalance_size', 'reference_price', 'matched_size', 'wap'];\n",
    "    ewm_cols  = [\"wap\"];\n",
    "\n",
    "    df[\"imbalance_size\"] = df[\"imbalance_size\"] * df[\"imbalance_buy_sell_flag\"];\n",
    "    df[\"ratio_imb_mat\"] = df[\"imbalance_size\"] / df[\"matched_size\"];\n",
    "    df = df.drop(columns = [\"imbalance_buy_sell_flag\"]);\n",
    "\n",
    "    # Date and time calculation:-\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60;\n",
    "    df[\"minute\"]  = df[\"seconds_in_bucket\"] // 60;\n",
    "\n",
    "    # Global feature calculation:-\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict());\n",
    "\n",
    "    # RSI calculation:-\n",
    "    df = generate_rsi(df)\n",
    "\n",
    "    # General feature calculation:-\n",
    "    df[\"volume\"]              = df.eval(\"ask_size + bid_size\");\n",
    "    df[\"mid_price\"]           = df.eval(\"(ask_price + bid_price) / 2\");\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\");\n",
    "    df[\"matched_imbalance\"]   = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\");\n",
    "    df[\"size_imbalance\"]      = df.eval(\"bid_size / ask_size\");\n",
    "    df['price_diff']          = df['reference_price'] - df['wap'];\n",
    "    df[\"imbalance_momentum\"]  = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size'];\n",
    "    df[\"price_spread\"]        = df[\"ask_price\"] - df[\"bid_price\"];\n",
    "    df[\"spread_intensity\"]    = df.groupby(['stock_id'])['price_spread'].diff();\n",
    "    df['price_pressure']      = df['imbalance_size'] * (df['ask_price'] - df['bid_price']);\n",
    "    df['market_urgency']      = df['price_spread'] * df['liquidity_imbalance'];\n",
    "    df['depth_pressure']      = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price']);\n",
    "    df['spread_depth_ratio']  = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size']);\n",
    "    df['mid_price_movement']  = df.groupby([\"stock_id\"])['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0));\n",
    "    df['micro_price']         = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size']);\n",
    "    df['relative_spread']     = (df['ask_price'] - df['bid_price']) / df['wap'];\n",
    "    df['high_volume']         = np.where(df['volume'] > df['global_median_size'], 1, 0);\n",
    "\n",
    "    # Combination feature calculation:-\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\");\n",
    "\n",
    "    # Distribution feature calculation:-\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "\n",
    "    for col in roll_cols:\n",
    "        for window in roll_window:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window);\n",
    "            df[f\"{col}_ret_{window}\"]   = df.groupby('stock_id')[col].pct_change(window);\n",
    "\n",
    "    if wap_ftre_req == \"Y\":\n",
    "        for feature in ['imbalance_size', 'reference_price', 'matched_size', 'wap']:\n",
    "            for window_size in ma_window:\n",
    "                df[f'{feature}_rolling_mean_{window_size}'] = \\\n",
    "                df.groupby('stock_id')[feature].\\\n",
    "                transform(lambda x: x.rolling(window=window_size, min_periods=window_size).mean());\n",
    "            for window_size in [20]:\n",
    "                df[f'{feature}_rolling_std_{window_size}'] = \\\n",
    "                df.groupby('stock_id')[feature].\\\n",
    "                transform(lambda x: x.rolling(window=window_size, min_periods=window_size).std());\n",
    "\n",
    "            # WAP feature calculation:-\n",
    "            for feature in ['wap']:\n",
    "                short_window, long_window = 12, 26\n",
    "                for window in [short_window, long_window]:\n",
    "                    df[f'{feature}_ewm_{window}'] = \\\n",
    "                    df.groupby('stock_id')[feature].transform(lambda x: x.ewm(span=window).mean())\n",
    "\n",
    "            df[f'{feature}_vol_st'] = \\\n",
    "            df.groupby(['stock_id'])[feature].pct_change().transform(lambda x: x.rolling(window=short_window).std());\n",
    "            df[f'{feature}_std_st'] = \\\n",
    "            df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=short_window).std());\n",
    "\n",
    "            df[f'{feature}_macd'] = df[f'{feature}_ewm_{short_window}'] - df[f'{feature}_ewm_{long_window}'];\n",
    "\n",
    "            # Bollinger Bands calculation:-\n",
    "            df[f'{feature}_bollinger_upper'] =\\\n",
    "            df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).mean()) + \\\n",
    "            2 * df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).std());\n",
    "\n",
    "            df[f'{feature}_bollinger_lower'] = \\\n",
    "            df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).mean()) - \\\n",
    "            2 * df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).std());\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0);\n",
    "\n",
    "gc.collect();\n",
    "print();\n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d370b020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:47.548557Z",
     "iopub.status.busy": "2024-07-03T03:45:47.548262Z",
     "iopub.status.idle": "2024-07-03T03:45:47.657085Z",
     "shell.execute_reply": "2024-07-03T03:45:47.656083Z"
    },
    "papermill": {
     "duration": 0.116995,
     "end_time": "2024-07-03T03:45:47.658959",
     "exception": false,
     "start_time": "2024-07-03T03:45:47.541964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mOnline mode\u001b[0m\n",
      "\n",
      "CPU times: user 104 ms, sys: 8 Âµs, total: 104 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if is_offline:\n",
    "    df_train = df[df[\"date_id\"] <= split_day];\n",
    "    df_valid = df[df[\"date_id\"] > split_day];\n",
    "    PrintColor(\"Offline mode\")\n",
    "    PrintColor(f\"train : {df_train.shape}, valid : {df_valid.shape}\");\n",
    "else:\n",
    "    df_train = df\n",
    "    PrintColor(\"Online mode\");\n",
    "    \n",
    "print();\n",
    "gc.collect();\n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d32bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:47.670996Z",
     "iopub.status.busy": "2024-07-03T03:45:47.670501Z",
     "iopub.status.idle": "2024-07-03T03:45:47.781216Z",
     "shell.execute_reply": "2024-07-03T03:45:47.780324Z"
    },
    "papermill": {
     "duration": 0.118917,
     "end_time": "2024-07-03T03:45:47.783222",
     "exception": false,
     "start_time": "2024-07-03T03:45:47.664305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 99.6 ms, sys: 965 Âµs, total: 101 ms\n",
      "Wall time: 99.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    \n",
    "    if is_offline:\n",
    "        df_train       = reduce_mem_usage(df_train);\n",
    "        df_train_feats = generate_all_features(df_train, global_stock_id_feats);\n",
    "        df_train_feats = reduce_mem_usage(df_train_feats);\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid = reduce_mem_usage(df_valid)\n",
    "        df_valid_feats = generate_all_features(df_valid, global_stock_id_feats)\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        \n",
    "    else:\n",
    "        df_train       = reduce_mem_usage(df_train)\n",
    "        df_train_feats = generate_all_features(df_train, global_stock_id_feats)\n",
    "        df_train_feats = reduce_mem_usage(df_train_feats)\n",
    "        print(\"Build Online Train Feats Finished.\");\n",
    "        \n",
    "gc.collect();\n",
    "print();\n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc80bb57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:47.795655Z",
     "iopub.status.busy": "2024-07-03T03:45:47.795383Z",
     "iopub.status.idle": "2024-07-03T03:45:47.931596Z",
     "shell.execute_reply": "2024-07-03T03:45:47.930581Z"
    },
    "papermill": {
     "duration": 0.144806,
     "end_time": "2024-07-03T03:45:47.933533",
     "exception": false,
     "start_time": "2024-07-03T03:45:47.788727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 103 ms, sys: 1.72 ms, total: 105 ms\n",
      "Wall time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if is_train:\n",
    "    feature_name = list(df_train_feats.columns)\n",
    "\n",
    "    lgb_params = {\n",
    "        'objective': 'mae', \n",
    "        'random_state': 42,\n",
    "        'device': 'gpu',\n",
    "        'boosting_type': 'gbdt', \n",
    "        'learning_rate': 0.015, \n",
    "        'max_depth': 12, \n",
    "        'n_estimators': 2000 if inference_flag == 0 else 2250, \n",
    "        'num_leaves': 300, \n",
    "        'reg_alpha': 0.005, \n",
    "        'reg_lambda': 0.001, \n",
    "        'colsample_bytree': 0.6, \n",
    "        'subsample': 0.875, \n",
    "        'min_child_samples': 128,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cat_params = dict(iterations=5042 if inference_flag == 0 else 5600,\n",
    "                      learning_rate=0.31464616673879614,\n",
    "                      depth=9,\n",
    "                      l2_leaf_reg=15.775786106845466,\n",
    "                      bootstrap_type='Bernoulli',\n",
    "                      subsample=0.9238669922301226,\n",
    "                      loss_function='MAE',\n",
    "                      eval_metric = 'MAE',\n",
    "                      metric_period=1000,\n",
    "                      task_type='GPU',\n",
    "                      allow_writing_files=False,\n",
    "                      random_state=42\n",
    "                      )\n",
    "    \n",
    "    xgb_params = {    'tree_method'           : 'hist',\n",
    "                      'device'                : \"cuda\",\n",
    "                      'objective'             : 'reg:absoluteerror',\n",
    "                      'n_estimators'          : 1800,\n",
    "                      'eval_metric'           : 'mae',\n",
    "                      'learning_rate'         : 0.018,\n",
    "                      'max_depth'             : 11,\n",
    "                      'colsample_bytree'      : 0.65,\n",
    "                      'reg_alpha'             : 0.001,\n",
    "                      'reg_lambda'            : 0.005,\n",
    "                      'verbosity'             : 0,\n",
    "                      'random_state'          : 42,\n",
    "                     }\n",
    "    \n",
    "    \n",
    "    print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "    # infer\n",
    "    df_train_target = df_train[\"target\"]\n",
    "    print(\"Infer Model Trainning.\")\n",
    "   \n",
    "    if use_lgb:\n",
    "        infer_lgb_params = lgb_params.copy()\n",
    "        print()\n",
    "        PrintColor(\"---> Infer LGB Params\", color=Fore.MAGENTA)\n",
    "        print(infer_lgb_params)\n",
    "        infer_lgb_model = lgb.LGBMRegressor(**infer_lgb_params)\n",
    "        infer_lgb_model.fit(df_train_feats[feature_name], df_train_target)\n",
    "        joblib.dump(infer_lgb_model, f'LGB_v{version}.model');\n",
    "    \n",
    "    if use_cat:\n",
    "        infer_cat_params = cat_params.copy()\n",
    "        print()\n",
    "        PrintColor(\"---> Infer Cat Params\", color=Fore.MAGENTA)\n",
    "        print(infer_cat_params)\n",
    "        infer_cat_model = CatBoostRegressor(**infer_cat_params)\n",
    "        infer_cat_model.fit(df_train_feats[feature_name], df_train_target)\n",
    "        joblib.dump(infer_cat_model, f'CAT_v{version}.model');\n",
    "        \n",
    "    if use_xgb:\n",
    "        infer_xgb_params = xgb_params.copy()\n",
    "        print()\n",
    "        PrintColor(\"---> Infer XGB Params\", color=Fore.MAGENTA)\n",
    "        print(infer_xgb_params)\n",
    "        infer_xgb_model = XGBRegressor(**infer_xgb_params)\n",
    "        infer_xgb_model.fit(df_train_feats[feature_name], df_train_target)\n",
    "        joblib.dump(infer_xgb_model, f'XGB_v{version}.model');\n",
    "\n",
    "    if is_offline:\n",
    "        if not streaming:\n",
    "            # offline predictions\n",
    "            df_valid_target = df_valid[\"target\"]\n",
    "\n",
    "            if use_lgb:\n",
    "                offline_predictions_lgb = infer_lgb_model.predict(df_valid_feats[feature_name])\n",
    "            if use_cat:\n",
    "                offline_predictions_cat = infer_cat_model.predict(df_valid_feats[feature_name])\n",
    "            if use_xgb:\n",
    "                offline_predictions_xgb = infer_xgb_model.predict(df_valid_feats[feature_name])\n",
    "\n",
    "            offline_predictions = None\n",
    "    \n",
    "            if use_lgb:\n",
    "                if offline_predictions is None:\n",
    "                    offline_predictions = (lgb_weight * offline_predictions_lgb)\n",
    "                else:\n",
    "                    offline_predictions += (lgb_weight * offline_predictions_lgb)\n",
    "\n",
    "            if use_cat:\n",
    "                if offline_predictions is None:\n",
    "                    offline_predictions = (cat_weight * offline_predictions_cat)\n",
    "                else:\n",
    "                    offline_predictions += (cat_weight * offline_predictions_cat)\n",
    "\n",
    "            if use_xgb:\n",
    "                if offline_predictions is None:\n",
    "                    offline_predictions = (xgb_weight * offline_predictions_xgb)\n",
    "                else:\n",
    "                    offline_predictions += (xgb_weight * offline_predictions_xgb)\n",
    "\n",
    "            zero_sum_preds = zero_sum(offline_predictions, df_valid_feats['bid_size'] + df_valid_feats['ask_size'])\n",
    "            zero_mean_preds = offline_predictions - offline_predictions.mean()\n",
    "            offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n",
    "            zero_sum_score = mean_absolute_error(zero_sum_preds, df_valid_target)\n",
    "            zero_mean_score = mean_absolute_error(zero_mean_preds, df_valid_target)\n",
    "            PrintColor(f\"Offline Score {np.round(offline_score, 4)}\", color = Fore.CYAN)\n",
    "            PrintColor(f\"Zero Sum Score {np.round(zero_sum_score, 4)}\")\n",
    "            PrintColor(f\"Zero Mean Score {np.round(zero_mean_score, 4)}\", color=Fore.YELLOW)\n",
    "            \n",
    "        else:\n",
    "            # offline predictions\n",
    "            offline_predictions = []\n",
    "            zero_sum_preds = []\n",
    "            zero_mean_preds = []\n",
    "            df_valid_target = []\n",
    "            qps = []\n",
    "            cache = pd.DataFrame()\n",
    "            counter = 0\n",
    "            for dt in range(436, 481, 1):\n",
    "                for t in range(0, 550, 10):\n",
    "\n",
    "                    now_time = time.time()\n",
    "\n",
    "                    SUBMIT_TEST = pd.read_csv(INPUT_DIR / f\"stream-data/{dt}_{t}_val.csv\")\n",
    "                    df_valid_target_stream = SUBMIT_TEST[\"target\"].values\n",
    "\n",
    "                    cache = pd.concat([cache, SUBMIT_TEST], ignore_index=True, axis=0)\n",
    "                    if counter > 0:\n",
    "                        cache = cache.groupby(['stock_id', 'seconds_in_bucket']).tail(2).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "                    feat = generate_all_features(cache, global_stock_id_feats)[-len(SUBMIT_TEST):]\n",
    "                    feat = reduce_mem_usage(feat)\n",
    "\n",
    "                    if use_lgb:\n",
    "                        offline_predictions_lgb = infer_lgb_model.predict(feat)\n",
    "                    if use_cat:\n",
    "                        offline_predictions_cat = infer_cat_model.predict(feat)\n",
    "                    if use_xgb:\n",
    "                        offline_predictions_xgb = infer_xgb_model.predict(df_valid_feats[feature_name])\n",
    "\n",
    "                    offline_predictions_stream = None\n",
    "    \n",
    "                    if use_lgb:\n",
    "                        if offline_predictions_stream is None:\n",
    "                            offline_predictions_stream = (lgb_weight * offline_predictions_lgb)\n",
    "                        else:\n",
    "                            offline_predictions_stream += (lgb_weight * offline_predictions_lgb)\n",
    "\n",
    "                    if use_cat:\n",
    "                        if offline_predictions_stream is None:\n",
    "                            offline_predictions_stream = (cat_weight * offline_predictions_cat)\n",
    "                        else:\n",
    "                            offline_predictions_stream += (cat_weight * offline_predictions_cat)\n",
    "\n",
    "                    if use_xgb:\n",
    "                        if offline_predictions_stream is None:\n",
    "                            offline_predictions_stream = (xgb_weight * offline_predictions_xgb)\n",
    "                        else:\n",
    "                            offline_predictions_stream += (xgb_weight * offline_predictions_xgb)\n",
    "\n",
    "                    zero_sum_preds_stream = zero_sum(offline_predictions_stream, SUBMIT_TEST['bid_size'] + SUBMIT_TEST['ask_size'])\n",
    "                    zero_mean_preds_stream = offline_predictions_stream - offline_predictions_stream.mean()\n",
    "\n",
    "                    if counter == 1:\n",
    "                        print(np.mean(zero_mean_preds_stream))\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                    offline_predictions.extend(list(offline_predictions_stream))\n",
    "                    zero_sum_preds.extend(list(zero_sum_preds_stream))\n",
    "                    zero_mean_preds.extend(list(zero_mean_preds_stream))\n",
    "                    df_valid_target.extend(list(df_valid_target_stream))\n",
    "\n",
    "                    qps.append(time.time() - now_time)\n",
    "\n",
    "                    if counter % 10 == 0:\n",
    "                        print(cache.shape)\n",
    "                        print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "            offline_predictions = np.array(offline_predictions)\n",
    "            zero_sum_preds = np.array(zero_sum_preds)\n",
    "            zero_mean_preds = np.array(zero_mean_preds)\n",
    "            df_valid_target = np.array(df_valid_target)\n",
    "\n",
    "            offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n",
    "            zero_sum_score = mean_absolute_error(zero_sum_preds, df_valid_target)\n",
    "            zero_mean_score = mean_absolute_error(zero_mean_preds, df_valid_target)\n",
    "            PrintColor(f\"Offline Score {np.round(offline_score, 4)}\", color = Fore.CYAN)\n",
    "            PrintColor(f\"Zero Sum Score {np.round(zero_sum_score, 4)}\")\n",
    "            PrintColor(f\"Zero Mean Score {np.round(zero_mean_score, 4)}\", color=Fore.YELLOW)\n",
    "            \n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n",
    "gc.collect();\n",
    "print();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0adaf28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:47.945748Z",
     "iopub.status.busy": "2024-07-03T03:45:47.945252Z",
     "iopub.status.idle": "2024-07-03T03:45:58.248951Z",
     "shell.execute_reply": "2024-07-03T03:45:58.248064Z"
    },
    "papermill": {
     "duration": 10.31224,
     "end_time": "2024-07-03T03:45:58.251262",
     "exception": false,
     "start_time": "2024-07-03T03:45:47.939022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.6, device=&#x27;gpu&#x27;, learning_rate=0.015,\n",
       "              max_depth=12, min_child_samples=128, n_estimators=2250,\n",
       "              num_leaves=300, objective=&#x27;mae&#x27;, random_state=42, reg_alpha=0.005,\n",
       "              reg_lambda=0.001, subsample=0.875, verbosity=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.6, device=&#x27;gpu&#x27;, learning_rate=0.015,\n",
       "              max_depth=12, min_child_samples=128, n_estimators=2250,\n",
       "              num_leaves=300, objective=&#x27;mae&#x27;, random_state=42, reg_alpha=0.005,\n",
       "              reg_lambda=0.001, subsample=0.875, verbosity=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.6, device='gpu', learning_rate=0.015,\n",
       "              max_depth=12, min_child_samples=128, n_estimators=2250,\n",
       "              num_leaves=300, objective='mae', random_state=42, reg_alpha=0.005,\n",
       "              reg_lambda=0.001, subsample=0.875, verbosity=-1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7f630d9a6c80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[34m\n",
      "\n",
      "---> Selected columns = 170\n",
      "\u001b[0m\n",
      "array(['stock_id', 'seconds_in_bucket', 'imbalance_size', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'prev_1_target', 'prev_2_target', 'prev_3_target', 'ratio_imb_mat', 'seconds', 'minute', 'global_median_size', 'global_std_size',\n",
      "       'global_ptp_size', 'global_median_price', 'global_std_price', 'global_ptp_price', 'rsi_reference_price', 'rsi_far_price', 'rsi_near_price',\n",
      "       'rsi_ask_price', 'rsi_bid_price', 'rsi_wap', 'volume', 'mid_price', 'liquidity_imbalance', 'matched_imbalance', 'size_imbalance', 'price_diff',\n",
      "       'imbalance_momentum', 'price_spread', 'spread_intensity', 'price_pressure', 'market_urgency', 'depth_pressure', 'spread_depth_ratio',\n",
      "       'mid_price_movement', 'micro_price', 'relative_spread', 'high_volume', 'reference_price_far_price_imb', 'reference_price_near_price_imb',\n",
      "       'reference_price_ask_price_imb', 'reference_price_bid_price_imb', 'reference_price_wap_imb', 'far_price_near_price_imb', 'far_price_ask_price_imb',\n",
      "       'far_price_bid_price_imb', 'far_price_wap_imb', 'near_price_ask_price_imb', 'near_price_bid_price_imb', 'near_price_wap_imb', 'ask_price_bid_price_imb',\n",
      "       'ask_price_wap_imb', 'bid_price_wap_imb', 'all_prices_mean', 'all_sizes_mean', 'all_prices_std', 'all_sizes_std', 'all_prices_skew', 'all_sizes_skew',\n",
      "       'all_prices_kurt', 'all_sizes_kurt', 'matched_size_shift_1', 'matched_size_ret_1', 'matched_size_shift_2', 'matched_size_ret_2', 'matched_size_shift_3',\n",
      "       'matched_size_ret_3', 'matched_size_shift_5', 'matched_size_ret_5', 'matched_size_shift_10', 'matched_size_ret_10', 'imbalance_size_shift_1',\n",
      "       'imbalance_size_ret_1', 'imbalance_size_shift_2', 'imbalance_size_ret_2', 'imbalance_size_shift_3', 'imbalance_size_ret_3', 'imbalance_size_shift_5',\n",
      "       'imbalance_size_ret_5', 'imbalance_size_shift_10', 'imbalance_size_ret_10', 'reference_price_shift_1', 'reference_price_ret_1',\n",
      "       'reference_price_shift_2', 'reference_price_ret_2', 'reference_price_shift_3', 'reference_price_ret_3', 'reference_price_shift_5',\n",
      "       'reference_price_ret_5', 'reference_price_shift_10', 'reference_price_ret_10', 'ask_price_shift_1', 'ask_price_ret_1', 'ask_price_shift_2',\n",
      "       'ask_price_ret_2', 'ask_price_shift_3', 'ask_price_ret_3', 'ask_price_shift_5', 'ask_price_ret_5', 'ask_price_shift_10', 'ask_price_ret_10',\n",
      "       'bid_price_shift_1', 'bid_price_ret_1', 'bid_price_shift_2', 'bid_price_ret_2', 'bid_price_shift_3', 'bid_price_ret_3', 'bid_price_shift_5',\n",
      "       'bid_price_ret_5', 'bid_price_shift_10', 'bid_price_ret_10', 'ask_size_shift_1', 'ask_size_ret_1', 'ask_size_shift_2', 'ask_size_ret_2',\n",
      "       'ask_size_shift_3', 'ask_size_ret_3', 'ask_size_shift_5', 'ask_size_ret_5', 'ask_size_shift_10', 'ask_size_ret_10', 'bid_size_shift_1',\n",
      "       'bid_size_ret_1', 'bid_size_shift_2', 'bid_size_ret_2', 'bid_size_shift_3', 'bid_size_ret_3', 'bid_size_shift_5', 'bid_size_ret_5', 'bid_size_shift_10',\n",
      "       'bid_size_ret_10', 'wap_shift_1', 'wap_ret_1', 'wap_shift_2', 'wap_ret_2', 'wap_shift_3', 'wap_ret_3', 'wap_shift_5', 'wap_ret_5', 'wap_shift_10',\n",
      "       'wap_ret_10', 'near_price_shift_1', 'near_price_ret_1', 'near_price_shift_2', 'near_price_ret_2', 'near_price_shift_3', 'near_price_ret_3',\n",
      "       'near_price_shift_5', 'near_price_ret_5', 'near_price_shift_10', 'near_price_ret_10', 'far_price_shift_1', 'far_price_ret_1', 'far_price_shift_2',\n",
      "       'far_price_ret_2', 'far_price_shift_3', 'far_price_ret_3', 'far_price_shift_5', 'far_price_ret_5', 'far_price_shift_10', 'far_price_ret_10'],\n",
      "      dtype='<U30')\n",
      "\n",
      "CPU times: user 10.3 s, sys: 978 ms, total: 11.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if inference_flag == 3:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    };\n",
    "    \n",
    "    df_train_debug = df_train[df_train.date_id >= 470].reset_index(drop=True);\n",
    "    df_train_debug = reduce_mem_usage(df_train_debug);\n",
    "    df_train_feats = generate_all_features(df_train_debug, global_stock_id_feats);\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats);\n",
    "    df_train_feats = df_train_feats.drop(columns = ['date_id', 'time_id', \"row_id\", \"dow\", \"target\"], errors = \"ignore\");\n",
    "    \n",
    "    if use_lgb:\n",
    "        infer_lgb_model = joblib.load(f\"/kaggle/input/optivermodels/LGBM1R_V{version}.model\");\n",
    "        display(infer_lgb_model);\n",
    "        print();\n",
    "        \n",
    "    if use_cat:\n",
    "        infer_cat_model = joblib.load(f\"/kaggle/input/optivermodels/CBR_V{version}.model\");\n",
    "        display(infer_cat_model);\n",
    "        print();\n",
    "        \n",
    "    if use_xgb:\n",
    "        infer_xgb_model = joblib.load(f\"/kaggle/input/optivermodels/XGBR_V{version}.model\");\n",
    "        display(infer_xgb_model);\n",
    "        print();\n",
    "        \n",
    "    feature_name = list(df_train_feats.columns);\n",
    "    PrintColor(f\"\\n\\n---> Selected columns = {len(feature_name)}\\n\");\n",
    "    with np.printoptions(linewidth = 160):\n",
    "        pprint(np.array(feature_name));\n",
    "    \n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n",
    "gc.collect();\n",
    "print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83035d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:58.264728Z",
     "iopub.status.busy": "2024-07-03T03:45:58.264458Z",
     "iopub.status.idle": "2024-07-03T03:45:58.441608Z",
     "shell.execute_reply": "2024-07-03T03:45:58.440650Z"
    },
    "papermill": {
     "duration": 0.186106,
     "end_time": "2024-07-03T03:45:58.443655",
     "exception": false,
     "start_time": "2024-07-03T03:45:58.257549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 137 ms, sys: 1.76 ms, total: 139 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if is_infer:\n",
    "    has_refitted = False;\n",
    "    models_dict  = {};\n",
    "    \n",
    "    import optiver2023;\n",
    "    optiver2023.make_env.func_dict['__called__'] = False;\n",
    "    env       = optiver2023.make_env();\n",
    "    iter_test = env.iter_test();\n",
    "    counter         = 0;\n",
    "    next_refit_date = None;\n",
    "    lgb_refit_times = 0;\n",
    "    cat_refit_times = 0;\n",
    "    refit_times     = 0;\n",
    "    models_dict[f\"infer_lgb_{lgb_refit_times}\"] = infer_lgb_model;\n",
    "    models_dict[f\"infer_cat_{cat_refit_times}\"] = infer_cat_model;\n",
    "        \n",
    "    qps, predictions = 0, [];\n",
    "    \n",
    "    prev_df       = pd.DataFrame();\n",
    "    cache         = pd.DataFrame();\n",
    "    date_3_target = pd.DataFrame();\n",
    "    date_2_target = pd.DataFrame();\n",
    "    date_target   = pd.DataFrame();\n",
    "    \n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n",
    "gc.collect();\n",
    "print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbdc5fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T03:45:58.457495Z",
     "iopub.status.busy": "2024-07-03T03:45:58.457195Z",
     "iopub.status.idle": "2024-07-03T03:46:07.918509Z",
     "shell.execute_reply": "2024-07-03T03:46:07.917372Z"
    },
    "papermill": {
     "duration": 9.470852,
     "end_time": "2024-07-03T03:46:07.920656",
     "exception": false,
     "start_time": "2024-07-03T03:45:58.449804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "\u001b[1m\u001b[34m\n",
      "\n",
      "Submission file after refits\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480_540_0</td>\n",
       "      <td>-0.456110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480_540_1</td>\n",
       "      <td>-0.467563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480_540_2</td>\n",
       "      <td>-0.184897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480_540_3</td>\n",
       "      <td>-1.325634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480_540_4</td>\n",
       "      <td>-0.930613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>480_540_5</td>\n",
       "      <td>3.129571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>480_540_6</td>\n",
       "      <td>1.577288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>480_540_7</td>\n",
       "      <td>-1.625770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480_540_8</td>\n",
       "      <td>1.034535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>480_540_9</td>\n",
       "      <td>-0.568070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id    target\n",
       "0  480_540_0 -0.456110\n",
       "1  480_540_1 -0.467563\n",
       "2  480_540_2 -0.184897\n",
       "3  480_540_3 -1.325634\n",
       "4  480_540_4 -0.930613\n",
       "5  480_540_5  3.129571\n",
       "6  480_540_6  1.577288\n",
       "7  480_540_7 -1.625770\n",
       "8  480_540_8  1.034535\n",
       "9  480_540_9 -0.568070"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CPU times: user 10.4 s, sys: 698 ms, total: 11.1 s\n",
      "Wall time: 9.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if is_infer:\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        \n",
    "        if test.seconds_in_bucket.iloc[0]== 0:\n",
    "            date_3_target = date_2_target\n",
    "            date_2_target = date_target\n",
    "            date_target = revealed_targets\n",
    "            \n",
    "        previous_target = \\\n",
    "        date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n",
    "        rename(columns = {\"revealed_target\": \"prev_1_target\"});\n",
    "        \n",
    "        try:\n",
    "            previous_2_target = \\\n",
    "            date_2_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n",
    "            rename(columns = {\"revealed_target\": \"prev_2_target\"});\n",
    "            previous_2_target[\"date_id\"] = previous_2_target[\"date_id\"] + 1;\n",
    "        except:\n",
    "            previous_2_target = date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\"]];\n",
    "            previous_2_target[\"prev_2_target\"] = np.nan;\n",
    "            \n",
    "        try:\n",
    "            previous_3_target = \\\n",
    "            date_3_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n",
    "            rename(columns = {\"revealed_target\": \"prev_3_target\"});\n",
    "            previous_3_target[\"date_id\"] = previous_3_target[\"date_id\"] + 2;\n",
    "        except:\n",
    "            previous_3_target = date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\"]];\n",
    "            previous_3_target[\"prev_3_target\"] = np.nan;\n",
    "            \n",
    "        \n",
    "        SUBMIT_TEST = test.merge(previous_target, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n",
    "        SUBMIT_TEST = SUBMIT_TEST.merge(previous_2_target, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n",
    "        SUBMIT_TEST = SUBMIT_TEST.merge(previous_3_target, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n",
    "        SUBMIT_TEST[\"prev_1_target\"] = SUBMIT_TEST[\"prev_1_target\"].astype('float');\n",
    "        SUBMIT_TEST[\"prev_2_target\"] = SUBMIT_TEST[\"prev_2_target\"].astype('float');\n",
    "        SUBMIT_TEST[\"prev_3_target\"] = SUBMIT_TEST[\"prev_3_target\"].astype('float');\n",
    "        \n",
    "        columns_given = ['seconds_in_bucket', 'imbalance_size',\n",
    "                         'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
    "                         'far_price', 'near_price', 'bid_price', 'bid_size',\n",
    "                         'ask_price', 'ask_size', 'wap'\n",
    "                        ];\n",
    "        SUBMIT_TEST[columns_given] = SUBMIT_TEST[columns_given].astype('float');\n",
    "        \n",
    "        if test.seconds_in_bucket.iloc[0]== 0:\n",
    "            curr_date = test.date_id.iloc[0]\n",
    "            try:\n",
    "                curr_date = int(curr_date)\n",
    "            except:\n",
    "                pass\n",
    "            if curr_date >= 482:\n",
    "                # Using the revealed target to make a training instance:-\n",
    "                to_concat = \\\n",
    "                date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n",
    "                rename(columns = {\"revealed_target\": \"target\"});\n",
    "                to_concat[\"date_id\"] = to_concat[\"date_id\"] - 1;\n",
    "                prev_df = \\\n",
    "                prev_df.merge(to_concat, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n",
    "                prev_df[\"target\"] = prev_df[\"target\"].astype('float');\n",
    "                df_train = pd.concat([df_train, prev_df]).reset_index(drop=True);\n",
    "                \n",
    "                # Retaining limited dates to prevent OOM:-\n",
    "                df_train = df_train[df_train.date_id >= curr_date - wd_refits].dropna(subset=[\"target\"]).reset_index(drop=True)\n",
    "\n",
    "            prev_df = pd.DataFrame();\n",
    "            if is_refit:\n",
    "                if (next_refit_date is None and test.currently_scored.iloc[0] == True):\n",
    "                    next_refit_date = curr_date;\n",
    "\n",
    "                if curr_date == next_refit_date:\n",
    "                    if curr_date >= 482:\n",
    "                        global_stock_id_feats = {\n",
    "                            \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "                            \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "                            \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "                            \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "                            \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "                            \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "                        };\n",
    "                        \n",
    "                        PrintColor(\"\\nRefitting Feature Generation in progress\");\n",
    "                        df_train              = reduce_mem_usage(df_train);\n",
    "                        df_train_feats_refit  = generate_all_features(df_train, global_stock_id_feats);\n",
    "                        df_train_feats_refit  = reduce_mem_usage(df_train_feats_refit);\n",
    "                        df_train_refit_target = df_train[\"target\"];\n",
    "                        \n",
    "                        if refit_times == 0 or refit_times % 2 == 0:\n",
    "                            lgb_refit_times += 1;\n",
    "                            lgb_refit_params = \\\n",
    "                            {'objective'              : 'mae', \n",
    "                             'random_state'           : 42,\n",
    "                             'device'                 : 'gpu',\n",
    "                             'boosting_type'          : 'gbdt', \n",
    "                             'learning_rate'          : 0.015, \n",
    "                             'max_depth'              : 12, \n",
    "                             'n_estimators'           : 2250, \n",
    "                             'num_leaves'             : 300, \n",
    "                             'reg_alpha'              : 0.005, \n",
    "                             'reg_lambda'             : 0.001, \n",
    "                             'colsample_bytree'       : 0.6, \n",
    "                             'subsample'              : 0.875, \n",
    "                             'min_child_samples'      : 128,\n",
    "                             'verbose'                : -1,\n",
    "                            };\n",
    "                        \n",
    "                            print();\n",
    "                            PrintColor(\"Refitting in progress\");\n",
    "                            PrintColor(\"---> Refit LGB Params\", color=Fore.MAGENTA);\n",
    "                            print(lgb_refit_params);\n",
    "                            lgb_refit_model = lgb.LGBMRegressor(**lgb_refit_params);\n",
    "                            lgb_refit_model.fit(df_train_feats_refit[feature_name], df_train_refit_target);\n",
    "                            PrintColor(\"Refitting Done!\");\n",
    "                            models_dict[f\"infer_lgb_{lgb_refit_times}\"] = lgb_refit_model;\n",
    "                            print();\n",
    "                        \n",
    "                        if refit_times == 0 or refit_times % 2 == 1:\n",
    "                            cat_refit_times += 1;\n",
    "                            cat_refit_params = {'iterations'    : 5100, \n",
    "                                                'learning_rate' : 0.2774258427582013, \n",
    "                                                'depth'         : 10, \n",
    "                                                'l2_leaf_reg'   : 25.205435098066893, \n",
    "                                                'bootstrap_type': 'Bernoulli', \n",
    "                                                'subsample'     : 0.9405735849013803, \n",
    "                                                'loss_function' : 'MAE', \n",
    "                                                'eval_metric'   : 'MAE', \n",
    "                                                'metric_period' : 1000, \n",
    "                                                'task_type'     : 'GPU', \n",
    "                                                'allow_writing_files': False, \n",
    "                                                'random_state'       : 42,\n",
    "                                               };\n",
    "\n",
    "                            PrintColor(\"---> Refit CAT Params\", color=Fore.MAGENTA);\n",
    "                            print(cat_refit_params);\n",
    "                            cat_refit_model = CatBoostRegressor(**cat_refit_params);\n",
    "                            cat_refit_model.fit(df_train_feats_refit[feature_name], df_train_refit_target, verbose = 0);\n",
    "                            PrintColor(\"Refitting Done!\");\n",
    "                            models_dict[f\"infer_cat_{cat_refit_times}\"] = cat_refit_model;\n",
    "                            print();\n",
    "                            \n",
    "                        if refit_times == 0: \n",
    "                            refit_times += 2;\n",
    "                        else: \n",
    "                            refit_times += 1;  \n",
    "                        has_refitted = True;\n",
    "                    \n",
    "                    if refit_times < nb_refits: \n",
    "                        next_refit_date += freq_refits;               \n",
    "         \n",
    "        # Making secondary features in the test set:-        \n",
    "        prev_df = \\\n",
    "        pd.concat([prev_df, SUBMIT_TEST.drop(columns = [\"currently_scored\"])]).\\\n",
    "        reset_index(drop=True);\n",
    "        cache = pd.concat([cache, SUBMIT_TEST], ignore_index=True, axis=0);\n",
    "        \n",
    "        if counter > 0:\n",
    "            cache = \\\n",
    "            cache.groupby(['stock_id']).\\\n",
    "            tail(100).\\\n",
    "            sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True);\n",
    "            \n",
    "        if test.currently_scored.iloc[0]== False and (test.seconds_in_bucket.iloc[0] != 540 or test.date_id.iloc[0]!= 480):\n",
    "            sample_prediction['target'] = 0;\n",
    "            env.predict(sample_prediction);\n",
    "            counter += 1;\n",
    "            continue\n",
    "            \n",
    "        feat = generate_all_features(cache, global_stock_id_feats)[-len(test):];\n",
    "        feat = feat[feature_name];\n",
    "        \n",
    "        # Collating single model predictions:-        \n",
    "        lgb_prediction = None;\n",
    "        if has_refitted:\n",
    "            lgb_prediction = models_dict[f\"infer_lgb_{lgb_refit_times}\"].predict(feat);\n",
    "        else:\n",
    "            lgb_prediction = infer_lgb_model.predict(feat);\n",
    "   \n",
    "        cat_prediction = None;\n",
    "        if has_refitted:\n",
    "            cat_prediction = models_dict[f\"infer_cat_{cat_refit_times}\"].predict(feat);\n",
    "        else:\n",
    "            cat_prediction = infer_cat_model.predict(feat);\n",
    "\n",
    "        # Ensembling predictions:-      \n",
    "        ensemble_prediction = None\n",
    "        if ensemble_prediction is None:\n",
    "            ensemble_prediction = (lgb_weight * lgb_prediction);\n",
    "        else:\n",
    "            ensemble_prediction += (lgb_weight * lgb_prediction);\n",
    "\n",
    "        if ensemble_prediction is None:\n",
    "            ensemble_prediction = (cat_weight * cat_prediction);\n",
    "        else:\n",
    "            ensemble_prediction += (cat_weight * cat_prediction);\n",
    "\n",
    "        # Post-processing ensemble predictions:-          \n",
    "        if postprocess == \"zero_mean\":\n",
    "            ensemble_prediction = ensemble_prediction - np.mean(ensemble_prediction);\n",
    "        elif postprocess == \"zero_sum\":\n",
    "            ensemble_prediction = zero_sum(ensemble_prediction, test['bid_size'] + test['ask_size']);\n",
    "        else:\n",
    "            pass;\n",
    "          \n",
    "        # Making API predictions:-           \n",
    "        sample_prediction['target'] = ensemble_prediction;\n",
    "        env.predict(sample_prediction);\n",
    "        counter += 1;\n",
    "         \n",
    "    PrintColor(f\"\\n\\nSubmission file after refits\\n\");\n",
    "    display(sample_prediction.head(10));\n",
    "    print();\n",
    "    \n",
    "ctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n",
    "gc.collect();\n",
    "print();   "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 3755317,
     "sourceId": 6497264,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4085969,
     "sourceId": 7090651,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3857034,
     "sourceId": 7230606,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3856023,
     "sourceId": 7335130,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 170.696435,
   "end_time": "2024-07-03T03:46:10.272077",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-03T03:43:19.575642",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
