{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":6497264,"sourceType":"datasetVersion","datasetId":3755317},{"sourceId":7090651,"sourceType":"datasetVersion","datasetId":4085969},{"sourceId":7230606,"sourceType":"datasetVersion","datasetId":3857034},{"sourceId":7335130,"sourceType":"datasetVersion","datasetId":3856023}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":201.394075,"end_time":"2023-12-17T01:30:48.787542","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-17T01:27:27.393467","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> OPTIVER INFERENCE- 170 FEATURES <br><div> ","metadata":{"papermill":{"duration":0.009038,"end_time":"2023-12-17T01:27:31.86212","exception":false,"start_time":"2023-12-17T01:27:31.853082","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# **IMPORTS AND INSTALLATIONS**","metadata":{}},{"cell_type":"code","source":"%%time \n\nfrom IPython.display import clear_output;\nimport gc;\n\n!pip install /kaggle/input/lightgbm410/lightgbm-4.1.0-py3-none-manylinux_2_28_x86_64.whl -q;\n!pip install /kaggle/input/xgboost-2-0-0-whl/xgboost-2.0.2-py3-none-manylinux2014_x86_64.whl -q;\n!pip install /kaggle/input/catboost-1-2-2/catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl -q;\n\nclear_output();\nprint();","metadata":{"papermill":{"duration":113.52152,"end_time":"2023-12-17T01:29:25.392635","exception":false,"start_time":"2023-12-17T01:27:31.871115","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T08:31:09.940765Z","iopub.execute_input":"2023-12-18T08:31:09.941018Z","iopub.status.idle":"2023-12-18T08:33:05.957884Z","shell.execute_reply.started":"2023-12-18T08:31:09.940994Z","shell.execute_reply":"2023-12-18T08:33:05.956701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nimport ctypes, gc, os, time, warnings, joblib, lightgbm as lgb, catboost, numpy as np, pandas as pd, polars as pl;\nfrom warnings import simplefilter;\nwarnings.filterwarnings(\"ignore\");\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning);\n\nfrom itertools import combinations;\nfrom pprint import pprint;\nfrom catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm;\nfrom xgboost import XGBRegressor;\nfrom numba import njit, prange, jit;\nfrom sklearn.metrics import mean_absolute_error;\nfrom sklearn.model_selection import KFold, TimeSeriesSplit;\nfrom colorama import Fore, Style, init;\n\nclear_output();\n\nprint(f\"\\nCurrent CatBoost version = {catboost.__version__}\");\nprint(f\"Current LightGBM version = {lgb.__version__}\\n\");\ngc.collect();\nprint();","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":8.513045,"end_time":"2023-12-17T01:29:33.915802","exception":false,"start_time":"2023-12-17T01:29:25.402757","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T08:33:05.960191Z","iopub.execute_input":"2023-12-18T08:33:05.96067Z","iopub.status.idle":"2023-12-18T08:33:13.728662Z","shell.execute_reply.started":"2023-12-18T08:33:05.960631Z","shell.execute_reply":"2023-12-18T08:33:13.727745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nversion        = 27;\ninference_flag = 3;\n\nif inference_flag == 0:\n    is_offline = True;\n    is_train   = True;\n    is_infer   = False;\n    debug_flag = False;\n    \nelif inference_flag == 1:\n    is_offline = False;\n    is_train   = True;\n    is_infer   = True;\n    debug_flag = True;\n    \nelif inference_flag == 2:\n    is_offline = False;\n    is_train   = True;\n    is_infer   = True;\n    debug_flag = False;\n    \nelif inference_flag == 3:\n    is_offline = False;\n    is_train = False;\n    is_infer = True;\n    debug_flag = True;\n    \nmax_lookback = np.nan;\nsplit_day    = 435;\n\n# DEBUG PARAMS\nstreaming    = False;\ncheck_code   = False;\ntest_runtime = False;\n\n# ENSEMBLE AND REFIT PARAMS\nuse_lgb = True;\nuse_cat = True;\nuse_xgb = False;\n\nlgb_weight  = 0.50;\ncat_weight  = 0.50;\nxgb_weight  = 0;\nis_refit    = True;\nnb_refits   = 12;\nfreq_refits = 6;\nwd_refits   = 450;\n\n# POST-PROCESSING PARAMETERS:- (zero_sum/ zero_mean/ NA):-\npostprocess = \"NA\";\n\ndef zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes);\n    step      = np.sum(prices)/np.sum(std_error);\n    out       = prices-std_error*step;\n    return out;\n\ndef PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n    \"Prints color outputs using colorama using a text F-string\";\n    print(style + color + text + Style.RESET_ALL); \n    \ngc.collect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:33:13.730449Z","iopub.execute_input":"2023-12-18T08:33:13.730884Z","iopub.status.idle":"2023-12-18T08:33:13.833658Z","shell.execute_reply.started":"2023-12-18T08:33:13.730837Z","shell.execute_reply":"2023-12-18T08:33:13.832764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **VERSION DETAILS**","metadata":{}},{"cell_type":"markdown","source":"| Version | Description | Date| Refits x Freq| Best LB Score| Post-processing|\n|:-:|---|:-:|:-:|:-:|:-:|\n|27 | * 170 features <br> * Alternating refits between LGBM, CB <br> * Refitting window = 470 dates <br> * 60-40 ensemble weights | 17Dec2023 | 12x6| 5.3387|NA|\n|27 | * 170 features <br> * Alternating refits between LGBM, CB <br> * Refitting window = 500 dates <br> * 60-40 ensemble weights | 18Dec2023 | 12x6| |NA|\n|27 | * 170 features <br> * Alternating refits between LGBM, CB <br> * Refitting window = 450 dates <br> * 50-50 ensemble weights | 18Dec2023 | 12x6| |NA|","metadata":{}},{"cell_type":"markdown","source":"# **PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"%%time \n\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\");\n\n# Joining lagged targets:-\nlast_targets = \\\ndf[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"target\"]].\\\nrename(columns = {\"target\": \"prev_1_target\"});\nlast_targets[\"date_id\"] = last_targets[\"date_id\"] + 1;\ndf = df.merge(last_targets, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\ndel last_targets;\ngc.collect();\nctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n\nlast_targets_2 = \\\ndf[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"target\"]].\\\nrename(columns = {\"target\": \"prev_2_target\"});\nlast_targets_2[\"date_id\"] = last_targets_2[\"date_id\"] + 2\ndf = df.merge(last_targets_2, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\")\ndel last_targets_2;\ngc.collect();\nctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n\nlast_targets_3 = \\\ndf[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"target\"]].\\\nrename(columns = {\"target\": \"prev_3_target\"})\nlast_targets_3[\"date_id\"] = last_targets_3[\"date_id\"] + 3\ndf = df.merge(last_targets_3, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\")\ndel last_targets_3;\ngc.collect();\nctypes.CDLL(\"libc.so.6\").malloc_trim(0);\n\ndf = df.dropna(subset=[\"target\", \"prev_1_target\", \"prev_2_target\", \"prev_3_target\"], how=\"any\");\ndf.reset_index(drop=True, inplace=True);\nprint(df.shape);\n\nif check_code:\n    df = df[df.date_id >= 375].reset_index(drop=True);\n\ngc.collect();\nprint();\nctypes.CDLL(\"libc.so.6\").malloc_trim(0);","metadata":{"papermill":{"duration":18.963694,"end_time":"2023-12-17T01:29:52.959514","exception":false,"start_time":"2023-12-17T01:29:33.99582","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T08:33:13.835845Z","iopub.execute_input":"2023-12-18T08:33:13.836125Z","iopub.status.idle":"2023-12-18T08:33:40.392852Z","shell.execute_reply.started":"2023-12-18T08:33:13.836101Z","shell.execute_reply":"2023-12-18T08:33:40.391844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **FEATURE ENGINEERING**","metadata":{"papermill":{"duration":0.010746,"end_time":"2023-12-17T01:30:21.153362","exception":false,"start_time":"2023-12-17T01:30:21.142616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time \n\n####################################################################\n# Memory reduction:-\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type to reduce memory usage.\n    \"\"\";\n\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object and col != \"target\":\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == \"int\" or str(col_type)[:4] == \"uint\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n    return df;\n\n####################################################################\n# RSI calculation:-\ndef calculate_rsi(prices, period= 14):\n    rsi_values = np.zeros_like(prices);\n    for col in prange(prices.shape[1]):\n        price_data = prices[:, col];\n        delta      = np.zeros_like(price_data);\n        delta[1:]  = price_data[1:] - price_data[:-1];\n        gain       = pd.Series(np.where(delta > 0, delta, 0));\n        loss       = pd.Series(np.where(delta < 0, -delta, 0));\n        avg_gain   = gain.rolling(window=period,\n                                  min_periods=period).mean(engine='numba', engine_kwargs={\"parallel\": True});\n        avg_loss   = loss.rolling(window=period,\n                                  min_periods=period).mean(engine='numba', engine_kwargs={\"parallel\": True});\n        rs         = avg_gain / avg_loss;\n        rs         = rs.replace([np.inf, -np.inf], 1e-9);\n        rsi_values[:, col] = 100 - (100 / (1 + rs));\n    return rsi_values;\n\ndef generate_rsi(df):\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"];\n    sizes  = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"];\n\n    for stock_id, values in df.groupby(['stock_id'])[prices]:\n        columns = [f'rsi_{col}' for col in values.columns];\n        data    = calculate_rsi(values.values);\n        df.loc[values.index, columns] = data;\n    return df;\n\ndef generate_all_features(df, global_ftre: dict,\n                          grouper_cols: list = ['stock_id'],\n                          roll_window : list = [1, 2, 3, 5, 10],\n                          ma_window   : list = [5, 10, 20],\n                          ewm_window  : list = [7, 30],\n                          wap_ftre_req: str = \"N\",\n                          **kwarg\n                          ):\n    \"\"\"\n    This function generates all secondary features for the model\n\n    Note:-\n    1. We make MACD, EWM and BBbands, ASHI index for WAP only\n    2. We make SMA for certain columns\n    3. We use global features also\n    \"\"\";\n\n    cols      = [c for c in df.columns if c not in [\"row_id\", \"time_id\"]];\n    df        = df[cols];\n    prices    = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"];\n    sizes     = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"];\n    roll_cols = ['matched_size', 'imbalance_size', 'reference_price',\n                 'ask_price', 'bid_price', 'ask_size', 'bid_size', 'wap', 'near_price', 'far_price']\n    ma_cols   = ['imbalance_size', 'reference_price', 'matched_size', 'wap'];\n    ewm_cols  = [\"wap\"];\n\n    df[\"imbalance_size\"] = df[\"imbalance_size\"] * df[\"imbalance_buy_sell_flag\"];\n    df[\"ratio_imb_mat\"] = df[\"imbalance_size\"] / df[\"matched_size\"];\n    df = df.drop(columns = [\"imbalance_buy_sell_flag\"]);\n\n    # Date and time calculation:-\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60;\n    df[\"minute\"]  = df[\"seconds_in_bucket\"] // 60;\n\n    # Global feature calculation:-\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict());\n\n    # RSI calculation:-\n    df = generate_rsi(df)\n\n    # General feature calculation:-\n    df[\"volume\"]              = df.eval(\"ask_size + bid_size\");\n    df[\"mid_price\"]           = df.eval(\"(ask_price + bid_price) / 2\");\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\");\n    df[\"matched_imbalance\"]   = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\");\n    df[\"size_imbalance\"]      = df.eval(\"bid_size / ask_size\");\n    df['price_diff']          = df['reference_price'] - df['wap'];\n    df[\"imbalance_momentum\"]  = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size'];\n    df[\"price_spread\"]        = df[\"ask_price\"] - df[\"bid_price\"];\n    df[\"spread_intensity\"]    = df.groupby(['stock_id'])['price_spread'].diff();\n    df['price_pressure']      = df['imbalance_size'] * (df['ask_price'] - df['bid_price']);\n    df['market_urgency']      = df['price_spread'] * df['liquidity_imbalance'];\n    df['depth_pressure']      = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price']);\n    df['spread_depth_ratio']  = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size']);\n    df['mid_price_movement']  = df.groupby([\"stock_id\"])['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0));\n    df['micro_price']         = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size']);\n    df['relative_spread']     = (df['ask_price'] - df['bid_price']) / df['wap'];\n    df['high_volume']         = np.where(df['volume'] > df['global_median_size'], 1, 0);\n\n    # Combination feature calculation:-\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\");\n\n    # Distribution feature calculation:-\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n\n    for col in roll_cols:\n        for window in roll_window:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window);\n            df[f\"{col}_ret_{window}\"]   = df.groupby('stock_id')[col].pct_change(window);\n\n    if wap_ftre_req == \"Y\":\n        for feature in ['imbalance_size', 'reference_price', 'matched_size', 'wap']:\n            for window_size in ma_window:\n                df[f'{feature}_rolling_mean_{window_size}'] = \\\n                df.groupby('stock_id')[feature].\\\n                transform(lambda x: x.rolling(window=window_size, min_periods=window_size).mean());\n            for window_size in [20]:\n                df[f'{feature}_rolling_std_{window_size}'] = \\\n                df.groupby('stock_id')[feature].\\\n                transform(lambda x: x.rolling(window=window_size, min_periods=window_size).std());\n\n            # WAP feature calculation:-\n            for feature in ['wap']:\n                short_window, long_window = 12, 26\n                for window in [short_window, long_window]:\n                    df[f'{feature}_ewm_{window}'] = \\\n                    df.groupby('stock_id')[feature].transform(lambda x: x.ewm(span=window).mean())\n\n            df[f'{feature}_vol_st'] = \\\n            df.groupby(['stock_id'])[feature].pct_change().transform(lambda x: x.rolling(window=short_window).std());\n            df[f'{feature}_std_st'] = \\\n            df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=short_window).std());\n\n            df[f'{feature}_macd'] = df[f'{feature}_ewm_{short_window}'] - df[f'{feature}_ewm_{long_window}'];\n\n            # Bollinger Bands calculation:-\n            df[f'{feature}_bollinger_upper'] =\\\n            df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).mean()) + \\\n            2 * df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).std());\n\n            df[f'{feature}_bollinger_lower'] = \\\n            df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).mean()) - \\\n            2 * df.groupby(['stock_id'])[feature].transform(lambda x: x.rolling(window=long_window).std());\n\n    return df.replace([np.inf, -np.inf], 0);\n\ngc.collect();\nprint();\nctypes.CDLL(\"libc.so.6\").malloc_trim(0);","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:33:40.394559Z","iopub.execute_input":"2023-12-18T08:33:40.395352Z","iopub.status.idle":"2023-12-18T08:33:40.539778Z","shell.execute_reply.started":"2023-12-18T08:33:40.395317Z","shell.execute_reply":"2023-12-18T08:33:40.538892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nif is_offline:\n    df_train = df[df[\"date_id\"] <= split_day];\n    df_valid = df[df[\"date_id\"] > split_day];\n    PrintColor(\"Offline mode\")\n    PrintColor(f\"train : {df_train.shape}, valid : {df_valid.shape}\");\nelse:\n    df_train = df\n    PrintColor(\"Online mode\");\n    \nprint();\ngc.collect();\nctypes.CDLL(\"libc.so.6\").malloc_trim(0);","metadata":{"papermill":{"duration":0.01905,"end_time":"2023-12-17T01:30:22.542881","exception":false,"start_time":"2023-12-17T01:30:22.523831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T08:33:40.541013Z","iopub.execute_input":"2023-12-18T08:33:40.541388Z","iopub.status.idle":"2023-12-18T08:33:40.647865Z","shell.execute_reply.started":"2023-12-18T08:33:40.541354Z","shell.execute_reply":"2023-12-18T08:33:40.646858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    \n    if is_offline:\n        df_train       = reduce_mem_usage(df_train);\n        df_train_feats = generate_all_features(df_train, global_stock_id_feats);\n        df_train_feats = reduce_mem_usage(df_train_feats);\n        print(\"Build Train Feats Finished.\")\n        df_valid = reduce_mem_usage(df_valid)\n        df_valid_feats = generate_all_features(df_valid, global_stock_id_feats)\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n        print(\"Build Valid Feats Finished.\")\n        \n    else:\n        df_train       = reduce_mem_usage(df_train)\n        df_train_feats = generate_all_features(df_train, global_stock_id_feats)\n        df_train_feats = reduce_mem_usage(df_train_feats)\n        print(\"Build Online Train Feats Finished.\");\n        \ngc.collect();\nprint();\nctypes.CDLL(\"libc.so.6\").malloc_trim(0);","metadata":{"papermill":{"duration":0.022768,"end_time":"2023-12-17T01:30:22.612233","exception":false,"start_time":"2023-12-17T01:30:22.589465","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T08:33:40.649211Z","iopub.execute_input":"2023-12-18T08:33:40.649558Z","iopub.status.idle":"2023-12-18T08:33:40.763867Z","shell.execute_reply.started":"2023-12-18T08:33:40.649528Z","shell.execute_reply":"2023-12-18T08:33:40.762914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL TRAINING**","metadata":{}},{"cell_type":"code","source":"%%time \n\nif is_train:\n    feature_name = list(df_train_feats.columns)\n\n    lgb_params = {\n        'objective': 'mae', \n        'random_state': 42,\n        'device': 'gpu',\n        'boosting_type': 'gbdt', \n        'learning_rate': 0.015, \n        'max_depth': 12, \n        'n_estimators': 2000 if inference_flag == 0 else 2250, \n        'num_leaves': 300, \n        'reg_alpha': 0.005, \n        'reg_lambda': 0.001, \n        'colsample_bytree': 0.6, \n        'subsample': 0.875, \n        'min_child_samples': 128,\n    }\n    \n    \n    cat_params = dict(iterations=5042 if inference_flag == 0 else 5600,\n                      learning_rate=0.31464616673879614,\n                      depth=9,\n                      l2_leaf_reg=15.775786106845466,\n                      bootstrap_type='Bernoulli',\n                      subsample=0.9238669922301226,\n                      loss_function='MAE',\n                      eval_metric = 'MAE',\n                      metric_period=1000,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      random_state=42\n                      )\n    \n    xgb_params = {    'tree_method'           : 'hist',\n                      'device'                : \"cuda\",\n                      'objective'             : 'reg:absoluteerror',\n                      'n_estimators'          : 1800,\n                      'eval_metric'           : 'mae',\n                      'learning_rate'         : 0.018,\n                      'max_depth'             : 11,\n                      'colsample_bytree'      : 0.65,\n                      'reg_alpha'             : 0.001,\n                      'reg_lambda'            : 0.005,\n                      'verbosity'             : 0,\n                      'random_state'          : 42,\n                     }\n    \n    \n    print(f\"Feature length = {len(feature_name)}\")\n\n    # infer\n    df_train_target = df_train[\"target\"]\n    print(\"Infer Model Trainning.\")\n   \n    if use_lgb:\n        infer_lgb_params = lgb_params.copy()\n        print()\n        PrintColor(\"---> Infer LGB Params\", color=Fore.MAGENTA)\n        print(infer_lgb_params)\n        infer_lgb_model = lgb.LGBMRegressor(**infer_lgb_params)\n        infer_lgb_model.fit(df_train_feats[feature_name], df_train_target)\n        joblib.dump(infer_lgb_model, f'LGB_v{version}.model');\n    \n    if use_cat:\n        infer_cat_params = cat_params.copy()\n        print()\n        PrintColor(\"---> Infer Cat Params\", color=Fore.MAGENTA)\n        print(infer_cat_params)\n        infer_cat_model = CatBoostRegressor(**infer_cat_params)\n        infer_cat_model.fit(df_train_feats[feature_name], df_train_target)\n        joblib.dump(infer_cat_model, f'CAT_v{version}.model');\n        \n    if use_xgb:\n        infer_xgb_params = xgb_params.copy()\n        print()\n        PrintColor(\"---> Infer XGB Params\", color=Fore.MAGENTA)\n        print(infer_xgb_params)\n        infer_xgb_model = XGBRegressor(**infer_xgb_params)\n        infer_xgb_model.fit(df_train_feats[feature_name], df_train_target)\n        joblib.dump(infer_xgb_model, f'XGB_v{version}.model');\n\n    if is_offline:\n        if not streaming:\n            # offline predictions\n            df_valid_target = df_valid[\"target\"]\n\n            if use_lgb:\n                offline_predictions_lgb = infer_lgb_model.predict(df_valid_feats[feature_name])\n            if use_cat:\n                offline_predictions_cat = infer_cat_model.predict(df_valid_feats[feature_name])\n            if use_xgb:\n                offline_predictions_xgb = infer_xgb_model.predict(df_valid_feats[feature_name])\n\n            offline_predictions = None\n    \n            if use_lgb:\n                if offline_predictions is None:\n                    offline_predictions = (lgb_weight * offline_predictions_lgb)\n                else:\n                    offline_predictions += (lgb_weight * offline_predictions_lgb)\n\n            if use_cat:\n                if offline_predictions is None:\n                    offline_predictions = (cat_weight * offline_predictions_cat)\n                else:\n                    offline_predictions += (cat_weight * offline_predictions_cat)\n\n            if use_xgb:\n                if offline_predictions is None:\n                    offline_predictions = (xgb_weight * offline_predictions_xgb)\n                else:\n                    offline_predictions += (xgb_weight * offline_predictions_xgb)\n\n            zero_sum_preds = zero_sum(offline_predictions, df_valid_feats['bid_size'] + df_valid_feats['ask_size'])\n            zero_mean_preds = offline_predictions - offline_predictions.mean()\n            offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n            zero_sum_score = mean_absolute_error(zero_sum_preds, df_valid_target)\n            zero_mean_score = mean_absolute_error(zero_mean_preds, df_valid_target)\n            PrintColor(f\"Offline Score {np.round(offline_score, 4)}\", color = Fore.CYAN)\n            PrintColor(f\"Zero Sum Score {np.round(zero_sum_score, 4)}\")\n            PrintColor(f\"Zero Mean Score {np.round(zero_mean_score, 4)}\", color=Fore.YELLOW)\n            \n        else:\n            # offline predictions\n            offline_predictions = []\n            zero_sum_preds = []\n            zero_mean_preds = []\n            df_valid_target = []\n            qps = []\n            cache = pd.DataFrame()\n            counter = 0\n            for dt in range(436, 481, 1):\n                for t in range(0, 550, 10):\n\n                    now_time = time.time()\n\n                    SUBMIT_TEST = pd.read_csv(INPUT_DIR / f\"stream-data/{dt}_{t}_val.csv\")\n                    df_valid_target_stream = SUBMIT_TEST[\"target\"].values\n\n                    cache = pd.concat([cache, SUBMIT_TEST], ignore_index=True, axis=0)\n                    if counter > 0:\n                        cache = cache.groupby(['stock_id', 'seconds_in_bucket']).tail(2).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n\n                    feat = generate_all_features(cache, global_stock_id_feats)[-len(SUBMIT_TEST):]\n                    feat = reduce_mem_usage(feat)\n\n                    if use_lgb:\n                        offline_predictions_lgb = infer_lgb_model.predict(feat)\n                    if use_cat:\n                        offline_predictions_cat = infer_cat_model.predict(feat)\n                    if use_xgb:\n                        offline_predictions_xgb = infer_xgb_model.predict(df_valid_feats[feature_name])\n\n                    offline_predictions_stream = None\n    \n                    if use_lgb:\n                        if offline_predictions_stream is None:\n                            offline_predictions_stream = (lgb_weight * offline_predictions_lgb)\n                        else:\n                            offline_predictions_stream += (lgb_weight * offline_predictions_lgb)\n\n                    if use_cat:\n                        if offline_predictions_stream is None:\n                            offline_predictions_stream = (cat_weight * offline_predictions_cat)\n                        else:\n                            offline_predictions_stream += (cat_weight * offline_predictions_cat)\n\n                    if use_xgb:\n                        if offline_predictions_stream is None:\n                            offline_predictions_stream = (xgb_weight * offline_predictions_xgb)\n                        else:\n                            offline_predictions_stream += (xgb_weight * offline_predictions_xgb)\n\n                    zero_sum_preds_stream = zero_sum(offline_predictions_stream, SUBMIT_TEST['bid_size'] + SUBMIT_TEST['ask_size'])\n                    zero_mean_preds_stream = offline_predictions_stream - offline_predictions_stream.mean()\n\n                    if counter == 1:\n                        print(np.mean(zero_mean_preds_stream))\n\n                    counter += 1\n\n                    offline_predictions.extend(list(offline_predictions_stream))\n                    zero_sum_preds.extend(list(zero_sum_preds_stream))\n                    zero_mean_preds.extend(list(zero_mean_preds_stream))\n                    df_valid_target.extend(list(df_valid_target_stream))\n\n                    qps.append(time.time() - now_time)\n\n                    if counter % 10 == 0:\n                        print(cache.shape)\n                        print(counter, 'qps:', np.mean(qps))\n\n            offline_predictions = np.array(offline_predictions)\n            zero_sum_preds = np.array(zero_sum_preds)\n            zero_mean_preds = np.array(zero_mean_preds)\n            df_valid_target = np.array(df_valid_target)\n\n            offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n            zero_sum_score = mean_absolute_error(zero_sum_preds, df_valid_target)\n            zero_mean_score = mean_absolute_error(zero_mean_preds, df_valid_target)\n            PrintColor(f\"Offline Score {np.round(offline_score, 4)}\", color = Fore.CYAN)\n            PrintColor(f\"Zero Sum Score {np.round(zero_sum_score, 4)}\")\n            PrintColor(f\"Zero Mean Score {np.round(zero_mean_score, 4)}\", color=Fore.YELLOW)\n            \nctypes.CDLL(\"libc.so.6\").malloc_trim(0);\ngc.collect();\nprint();\n","metadata":{"papermill":{"duration":0.043386,"end_time":"2023-12-17T01:30:22.688975","exception":false,"start_time":"2023-12-17T01:30:22.645589","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T08:33:40.765596Z","iopub.execute_input":"2023-12-18T08:33:40.765922Z","iopub.status.idle":"2023-12-18T08:33:40.900647Z","shell.execute_reply.started":"2023-12-18T08:33:40.765896Z","shell.execute_reply":"2023-12-18T08:33:40.899794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL INFERENCING AND SUBMISSION**","metadata":{}},{"cell_type":"code","source":"%%time \n\nif inference_flag == 3:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    };\n    \n    df_train_debug = df_train[df_train.date_id >= 470].reset_index(drop=True);\n    df_train_debug = reduce_mem_usage(df_train_debug);\n    df_train_feats = generate_all_features(df_train_debug, global_stock_id_feats);\n    df_train_feats = reduce_mem_usage(df_train_feats);\n    df_train_feats = df_train_feats.drop(columns = ['date_id', 'time_id', \"row_id\", \"dow\", \"target\"], errors = \"ignore\");\n    \n    if use_lgb:\n        infer_lgb_model = joblib.load(f\"/kaggle/input/optivermodels/LGBM1R_V{version}.model\");\n        display(infer_lgb_model);\n        print();\n        \n    if use_cat:\n        infer_cat_model = joblib.load(f\"/kaggle/input/optivermodels/CBR_V{version}.model\");\n        display(infer_cat_model);\n        print();\n        \n    if use_xgb:\n        infer_xgb_model = joblib.load(f\"/kaggle/input/optivermodels/XGBR_V{version}.model\");\n        display(infer_xgb_model);\n        print();\n        \n    feature_name = list(df_train_feats.columns);\n    PrintColor(f\"\\n\\n---> Selected columns = {len(feature_name)}\\n\");\n    with np.printoptions(linewidth = 160):\n        pprint(np.array(feature_name));\n    \nctypes.CDLL(\"libc.so.6\").malloc_trim(0);\ngc.collect();\nprint();","metadata":{"papermill":{"duration":13.999715,"end_time":"2023-12-17T01:30:36.722055","exception":false,"start_time":"2023-12-17T01:30:22.72234","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-18T08:33:40.90169Z","iopub.execute_input":"2023-12-18T08:33:40.901969Z","iopub.status.idle":"2023-12-18T08:33:51.626798Z","shell.execute_reply.started":"2023-12-18T08:33:40.901945Z","shell.execute_reply":"2023-12-18T08:33:51.62591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nif is_infer:\n    has_refitted = False;\n    models_dict  = {};\n    \n    import optiver2023;\n    optiver2023.make_env.func_dict['__called__'] = False;\n    env       = optiver2023.make_env();\n    iter_test = env.iter_test();\n    counter         = 0;\n    next_refit_date = None;\n    lgb_refit_times = 0;\n    cat_refit_times = 0;\n    refit_times     = 0;\n    models_dict[f\"infer_lgb_{lgb_refit_times}\"] = infer_lgb_model;\n    models_dict[f\"infer_cat_{cat_refit_times}\"] = infer_cat_model;\n        \n    qps, predictions = 0, [];\n    \n    prev_df       = pd.DataFrame();\n    cache         = pd.DataFrame();\n    date_3_target = pd.DataFrame();\n    date_2_target = pd.DataFrame();\n    date_target   = pd.DataFrame();\n    \nctypes.CDLL(\"libc.so.6\").malloc_trim(0);\ngc.collect();\nprint();","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:33:51.629806Z","iopub.execute_input":"2023-12-18T08:33:51.630174Z","iopub.status.idle":"2023-12-18T08:33:51.791198Z","shell.execute_reply.started":"2023-12-18T08:33:51.63014Z","shell.execute_reply":"2023-12-18T08:33:51.790295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\nif is_infer:\n    for (test, revealed_targets, sample_prediction) in iter_test:\n        \n        if test.seconds_in_bucket.iloc[0]== 0:\n            date_3_target = date_2_target\n            date_2_target = date_target\n            date_target = revealed_targets\n            \n        previous_target = \\\n        date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n        rename(columns = {\"revealed_target\": \"prev_1_target\"});\n        \n        try:\n            previous_2_target = \\\n            date_2_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n            rename(columns = {\"revealed_target\": \"prev_2_target\"});\n            previous_2_target[\"date_id\"] = previous_2_target[\"date_id\"] + 1;\n        except:\n            previous_2_target = date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\"]];\n            previous_2_target[\"prev_2_target\"] = np.nan;\n            \n        try:\n            previous_3_target = \\\n            date_3_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n            rename(columns = {\"revealed_target\": \"prev_3_target\"});\n            previous_3_target[\"date_id\"] = previous_3_target[\"date_id\"] + 2;\n        except:\n            previous_3_target = date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\"]];\n            previous_3_target[\"prev_3_target\"] = np.nan;\n            \n        \n        SUBMIT_TEST = test.merge(previous_target, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n        SUBMIT_TEST = SUBMIT_TEST.merge(previous_2_target, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n        SUBMIT_TEST = SUBMIT_TEST.merge(previous_3_target, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n        SUBMIT_TEST[\"prev_1_target\"] = SUBMIT_TEST[\"prev_1_target\"].astype('float');\n        SUBMIT_TEST[\"prev_2_target\"] = SUBMIT_TEST[\"prev_2_target\"].astype('float');\n        SUBMIT_TEST[\"prev_3_target\"] = SUBMIT_TEST[\"prev_3_target\"].astype('float');\n        \n        columns_given = ['seconds_in_bucket', 'imbalance_size',\n                         'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n                         'far_price', 'near_price', 'bid_price', 'bid_size',\n                         'ask_price', 'ask_size', 'wap'\n                        ];\n        SUBMIT_TEST[columns_given] = SUBMIT_TEST[columns_given].astype('float');\n        \n        if test.seconds_in_bucket.iloc[0]== 0:\n            curr_date = test.date_id.iloc[0]\n            try:\n                curr_date = int(curr_date)\n            except:\n                pass\n            if curr_date >= 482:\n                # Using the revealed target to make a training instance:-\n                to_concat = \\\n                date_target[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"revealed_target\"]].\\\n                rename(columns = {\"revealed_target\": \"target\"});\n                to_concat[\"date_id\"] = to_concat[\"date_id\"] - 1;\n                prev_df = \\\n                prev_df.merge(to_concat, on = [\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how = \"left\");\n                prev_df[\"target\"] = prev_df[\"target\"].astype('float');\n                df_train = pd.concat([df_train, prev_df]).reset_index(drop=True);\n                \n                # Retaining limited dates to prevent OOM:-\n                df_train = df_train[df_train.date_id >= curr_date - wd_refits].dropna(subset=[\"target\"]).reset_index(drop=True)\n\n            prev_df = pd.DataFrame();\n            if is_refit:\n                if (next_refit_date is None and test.currently_scored.iloc[0] == True):\n                    next_refit_date = curr_date;\n\n                if curr_date == next_refit_date:\n                    if curr_date >= 482:\n                        global_stock_id_feats = {\n                            \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n                            \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n                            \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n                            \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n                            \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n                            \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n                        };\n                        \n                        PrintColor(\"\\nRefitting Feature Generation in progress\");\n                        df_train              = reduce_mem_usage(df_train);\n                        df_train_feats_refit  = generate_all_features(df_train, global_stock_id_feats);\n                        df_train_feats_refit  = reduce_mem_usage(df_train_feats_refit);\n                        df_train_refit_target = df_train[\"target\"];\n                        \n                        if refit_times == 0 or refit_times % 2 == 0:\n                            lgb_refit_times += 1;\n                            lgb_refit_params = \\\n                            {'objective'              : 'mae', \n                             'random_state'           : 42,\n                             'device'                 : 'gpu',\n                             'boosting_type'          : 'gbdt', \n                             'learning_rate'          : 0.015, \n                             'max_depth'              : 12, \n                             'n_estimators'           : 2250, \n                             'num_leaves'             : 300, \n                             'reg_alpha'              : 0.005, \n                             'reg_lambda'             : 0.001, \n                             'colsample_bytree'       : 0.6, \n                             'subsample'              : 0.875, \n                             'min_child_samples'      : 128,\n                             'verbose'                : -1,\n                            };\n                        \n                            print();\n                            PrintColor(\"Refitting in progress\");\n                            PrintColor(\"---> Refit LGB Params\", color=Fore.MAGENTA);\n                            print(lgb_refit_params);\n                            lgb_refit_model = lgb.LGBMRegressor(**lgb_refit_params);\n                            lgb_refit_model.fit(df_train_feats_refit[feature_name], df_train_refit_target);\n                            PrintColor(\"Refitting Done!\");\n                            models_dict[f\"infer_lgb_{lgb_refit_times}\"] = lgb_refit_model;\n                            print();\n                        \n                        if refit_times == 0 or refit_times % 2 == 1:\n                            cat_refit_times += 1;\n                            cat_refit_params = {'iterations'    : 5100, \n                                                'learning_rate' : 0.2774258427582013, \n                                                'depth'         : 10, \n                                                'l2_leaf_reg'   : 25.205435098066893, \n                                                'bootstrap_type': 'Bernoulli', \n                                                'subsample'     : 0.9405735849013803, \n                                                'loss_function' : 'MAE', \n                                                'eval_metric'   : 'MAE', \n                                                'metric_period' : 1000, \n                                                'task_type'     : 'GPU', \n                                                'allow_writing_files': False, \n                                                'random_state'       : 42,\n                                               };\n\n                            PrintColor(\"---> Refit CAT Params\", color=Fore.MAGENTA);\n                            print(cat_refit_params);\n                            cat_refit_model = CatBoostRegressor(**cat_refit_params);\n                            cat_refit_model.fit(df_train_feats_refit[feature_name], df_train_refit_target, verbose = 0);\n                            PrintColor(\"Refitting Done!\");\n                            models_dict[f\"infer_cat_{cat_refit_times}\"] = cat_refit_model;\n                            print();\n                            \n                        if refit_times == 0: \n                            refit_times += 2;\n                        else: \n                            refit_times += 1;  \n                        has_refitted = True;\n                    \n                    if refit_times < nb_refits: \n                        next_refit_date += freq_refits;               \n         \n        # Making secondary features in the test set:-        \n        prev_df = \\\n        pd.concat([prev_df, SUBMIT_TEST.drop(columns = [\"currently_scored\"])]).\\\n        reset_index(drop=True);\n        cache = pd.concat([cache, SUBMIT_TEST], ignore_index=True, axis=0);\n        \n        if counter > 0:\n            cache = \\\n            cache.groupby(['stock_id']).\\\n            tail(100).\\\n            sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True);\n            \n        if test.currently_scored.iloc[0]== False and (test.seconds_in_bucket.iloc[0] != 540 or test.date_id.iloc[0]!= 480):\n            sample_prediction['target'] = 0;\n            env.predict(sample_prediction);\n            counter += 1;\n            continue\n            \n        feat = generate_all_features(cache, global_stock_id_feats)[-len(test):];\n        feat = feat[feature_name];\n        \n        # Collating single model predictions:-        \n        lgb_prediction = None;\n        if has_refitted:\n            lgb_prediction = models_dict[f\"infer_lgb_{lgb_refit_times}\"].predict(feat);\n        else:\n            lgb_prediction = infer_lgb_model.predict(feat);\n   \n        cat_prediction = None;\n        if has_refitted:\n            cat_prediction = models_dict[f\"infer_cat_{cat_refit_times}\"].predict(feat);\n        else:\n            cat_prediction = infer_cat_model.predict(feat);\n\n        # Ensembling predictions:-      \n        ensemble_prediction = None\n        if ensemble_prediction is None:\n            ensemble_prediction = (lgb_weight * lgb_prediction);\n        else:\n            ensemble_prediction += (lgb_weight * lgb_prediction);\n\n        if ensemble_prediction is None:\n            ensemble_prediction = (cat_weight * cat_prediction);\n        else:\n            ensemble_prediction += (cat_weight * cat_prediction);\n\n        # Post-processing ensemble predictions:-          \n        if postprocess == \"zero_mean\":\n            ensemble_prediction = ensemble_prediction - np.mean(ensemble_prediction);\n        elif postprocess == \"zero_sum\":\n            ensemble_prediction = zero_sum(ensemble_prediction, test['bid_size'] + test['ask_size']);\n        else:\n            pass;\n          \n        # Making API predictions:-           \n        sample_prediction['target'] = ensemble_prediction;\n        env.predict(sample_prediction);\n        counter += 1;\n         \n    PrintColor(f\"\\n\\nSubmission file after refits\\n\");\n    display(sample_prediction.head(10));\n    print();\n    \nctypes.CDLL(\"libc.so.6\").malloc_trim(0);\ngc.collect();\nprint();   ","metadata":{"execution":{"iopub.status.busy":"2023-12-18T08:33:51.792803Z","iopub.execute_input":"2023-12-18T08:33:51.79309Z","iopub.status.idle":"2023-12-18T08:34:00.98009Z","shell.execute_reply.started":"2023-12-18T08:33:51.793064Z","shell.execute_reply":"2023-12-18T08:34:00.979141Z"},"trusted":true},"execution_count":null,"outputs":[]}]}